<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Leo Wood</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="《纽约时报》2012年2月的一篇专栏中所称，“大数据”时代已经降临，在商业、经济及其他领域中，决策将日益基于数据和分析而作出，而并非基于经验和直觉。随大数据时代一起来来临的，是越来越多的大数据工作岗位。在此，我们利用Python编程，抓取智联招聘、51job等网站上面有关大数据的工作岗位数据。 ##爬虫基础知识 ####数据来源网络爬虫的数据一般都来自服务器的响应结果，通常有html和json数据">
<meta property="og:type" content="article">
<meta property="og:title" content="Leo Wood">
<meta property="og:url" content="http://yoursite.com/2018/05/18/spider-recruits/index.html">
<meta property="og:site_name" content="Leo Wood">
<meta property="og:description" content="《纽约时报》2012年2月的一篇专栏中所称，“大数据”时代已经降临，在商业、经济及其他领域中，决策将日益基于数据和分析而作出，而并非基于经验和直觉。随大数据时代一起来来临的，是越来越多的大数据工作岗位。在此，我们利用Python编程，抓取智联招聘、51job等网站上面有关大数据的工作岗位数据。 ##爬虫基础知识 ####数据来源网络爬虫的数据一般都来自服务器的响应结果，通常有html和json数据">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/5759501-962ec23acb1bad2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/5759501-766c5b05dea62dd6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/5759501-fe83547efdccca87.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/5759501-c5fa43f51e6339eb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/5759501-92464313ba4cba7a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/5759501-00d99dc97d831f12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2018-05-17T16:36:06.998Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Leo Wood">
<meta name="twitter:description" content="《纽约时报》2012年2月的一篇专栏中所称，“大数据”时代已经降临，在商业、经济及其他领域中，决策将日益基于数据和分析而作出，而并非基于经验和直觉。随大数据时代一起来来临的，是越来越多的大数据工作岗位。在此，我们利用Python编程，抓取智联招聘、51job等网站上面有关大数据的工作岗位数据。 ##爬虫基础知识 ####数据来源网络爬虫的数据一般都来自服务器的响应结果，通常有html和json数据">
<meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/5759501-962ec23acb1bad2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
  
    <link rel="alternate" href="/atom.xml" title="Leo Wood" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Leo Wood</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-spider-recruits" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/05/18/spider-recruits/" class="article-date">
  <time datetime="2018-05-17T16:35:25.177Z" itemprop="datePublished">2018-05-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>《纽约时报》2012年2月的一篇专栏中所称，“大数据”时代已经降临，在商业、经济及其他领域中，决策将日益基于数据和分析而作出，而并非基于经验和直觉。随大数据时代一起来来临的，是越来越多的大数据工作岗位。在此，我们利用Python编程，抓取智联招聘、51job等网站上面有关大数据的工作岗位数据。</p>
<p>##爬虫基础知识</p>
<p>####数据来源<br>网络爬虫的数据一般都来自服务器的响应结果，通常有html和json数据等，这两种数据也是网络爬虫的主要数据来源。<br>其中html数据是网页的源代码，通过浏览器-查看源代码可以直接查看，例如：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/5759501-962ec23acb1bad2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="简书主页部分源码示例"></p>
<p>json是一种数据存储格式，往往包含了最原始的数据内容，一般不直接显示在网页中，这里我们可以通过Chrome浏览器-开发者工具中的Network选项捕获到服务器返回的json数据，例如：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/5759501-766c5b05dea62dd6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="简书首页json数据示例"></p>
<p>####数据请求<br>数据请求的方式一般有两种：GET方法和POST方法。我们也可以通过Chrome浏览器来捕获我们访问一个浏览器时的所有请求。这里以简书主页为例，打开Chrome浏览器开发者工具（F12），切换到Network选项，在地址栏输入<a href="http://www.jianshu.com/，" target="_blank" rel="noopener">http://www.jianshu.com/，</a> 选择XHR类型，可以看到一条请求的内容，打开Headers，在General中可以看到请求方式为GET方式，<br>其中的Request Headers便是我们访问这个网页时的请求数据，如下图。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/5759501-fe83547efdccca87.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Request Headers"></p>
<p>这个Headers可以用Python中的字典来表示，包含了用户请求的一些信息，例如编码、语言、用户登陆信息、浏览器信息等。<br>下面还有一个Query String Parameters，这里面包含了用户请求的一些参数，也是请求数据的一部分。</p>
<ul>
<li>利用requests库请求数据<br>利用Python构建数据请求的方式有很多，在python3中，主要有urllib和requests两个类库可以实现该功能。urllib是官方标准库，其官方文档<a href="https://docs.python.org/2/library/urllib.html" target="_blank" rel="noopener">传送门</a>。这里我们主要介绍第三方库requests，它是基于urllib编写的，比urllib用起来更加便捷，可以节约时间。<br>requests安装方法：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$  pip install requests</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>利用requests构建数据请求主要方式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">req = request.get(url)</span><br></pre></td></tr></table></figure></p>
<p>或者<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">req = requests.post(url)</span><br></pre></td></tr></table></figure></p>
<p>其中，get()与post()中都可以添加headers、params等参数，以字典的形式传递即可。一般来说，简单的网页通过传入url数据即可成功请求数据。不过一些网站采用了反爬虫机制，需要我们传入headers及params等参数，以模拟浏览器访问、用户登陆等行为，才可以正常请求数据。</p>
<ul>
<li>利用webdriver请求数据<br>webdriver是一个用来进行复杂重复的web自动化测试的工具，能够使用chrome、firefox、IE浏览器进行web测试，可以模拟用户点击链接，填写表单，点击按钮等。因此，相对于requests库来说，webdriver在模拟浏览器鼠标点击滑动等事件上有着天然的优势，并且真实模拟了浏览器的操作，不易被反爬虫机制发现，因此是一个很好用的爬虫工具。当然，其缺点在于速度较慢，效率不高。<br>webdriver安装：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install selnium</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>除了安装selnium库，webdriver的运行还需要进行浏览器驱动的配置。Chrome、火狐和IE浏览器都有其配置方式，具体方法查看链接<a href="http://blog.163.com/yang_jianli/blog/static/1619900062014102833427464/。" target="_blank" rel="noopener">http://blog.163.com/yang_jianli/blog/static/1619900062014102833427464/。</a><br>这里我们以IE浏览器为例，做一个简单的示范：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">import os</span><br><span class="line">iedriver = &quot;IEDriverServer.exe&quot;</span><br><span class="line">os.environ[&quot;webdriver.ie.driver&quot;] = iedriver</span><br><span class="line">driver = webdriver.Ie(iedriver)</span><br></pre></td></tr></table></figure></p>
<p>如此，IE浏览器配置完毕，其中”IEDriverServer.exe”是IE浏览器驱动的存储路径。<br>于是，我们我们访问简书网主页数据只一步：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.get(http://www.jianshu.com/)</span><br></pre></td></tr></table></figure></p>
<p>####数据解析<br>使用requests请求下来的数据，可以利用.text()方法或者.content()方法访问，对于文本请求，二者并无太大差别，主要在于编码问题。具体用法可以参考官方文档，这里不再赘述。使用webdriver请求下来的数据可以用.page_source属性获取。请求下来的数据一般包含了大量的网页源代码，如何将其解析以提取出我们想要的内容呢？</p>
<ul>
<li>html类型数据解析<br>html语言即超文本标记语言，它是由一个个html标签构成的，是结构化的语言，因此很容易从中匹配提取信息。这种类型的数据解析的方法有很多，比如利用正则表达式，按照html标签的结构进行字符串匹配，或则利用lxml库中的xpath方法使用xpath路径定位到每一个节点、也有类似jQuery的PyQuery方法。这里我们主要介绍BeautifulSoup方法。<br><a href="http://www.crummy.com/software/BeautifulSoup/" target="_blank" rel="noopener">Beautiful Soup</a> 是一个可以从HTML或XML文件中提取数据的Python库.它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式.Beautiful Soup会帮你节省数小时甚至数天的工作时间。该介绍来源于其官方中文文档，<a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/" target="_blank" rel="noopener">传送门</a>。利用BeautifulSoup我们能够将html字符串转化为树状结构，并非常快速地定位到每一个标签。<br>目前版本是BeautifulSoup4，pip安装方法：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install BeautifulSoup4</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>或者，下载bs4的<a href="https://www.crummy.com/software/BeautifulSoup/bs4/download/4.0/" target="_blank" rel="noopener">源码</a>，然后解压并运行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python setup.py install</span><br></pre></td></tr></table></figure></p>
<p>利用BeautifulSoup解析html数据的关键步骤为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(req.contents, &quot;html.parser&quot;)</span><br></pre></td></tr></table></figure></p>
<p>如果采用webdriver请求数据，那么则是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(driver.page_source, &quot;html.parser&quot;)</span><br></pre></td></tr></table></figure></p>
<p>  如此，便将html数据转换成BeautifulSoup中的树状结构。然后利用BeautifulSoup中的find()、find_all()等方法即可定位到每一个节点。详情请参阅官方文档。</p>
<ul>
<li>json类型数据解析<br>json类型的数据已经是高度结构化的数据，跟Python中字典的表示形式一样，因此在解析上十分方便。我们可以通过：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line">data = json.loads(req.text)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>直接读取json数据，且能够返回字典类型。</p>
<p>##大数据职位数据爬虫实战<br>这里我们以51job网站为例，构建大数据相关职位的数据爬虫。其中搜索关键词为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">数据科学家</span><br><span class="line">数据分析师</span><br><span class="line">数据架构师</span><br><span class="line">数据工程师</span><br><span class="line">统计学家</span><br><span class="line">数据库管理员</span><br><span class="line">业务数据分析师</span><br><span class="line">数据产品经理</span><br></pre></td></tr></table></figure></p>
<ul>
<li>网页分析<br>打开51job首页<a href="http://www.51job.com/，" target="_blank" rel="noopener">http://www.51job.com/，</a> 在搜索框中输入“数据科学家”，将搜索框中的地区点开，去掉当前勾选的城市，即默认在全国范围搜索。点击“搜索”按钮，得到搜索结果。这时我们将网址栏URL复制出来：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> http://search.51job.com/list/000000,000000,0000,00,9,99,</span><br><span class="line">%25E6%2595%25B0%25E6%258D%25AE%25E7%25A7%2591%25E5%25AD%25A6%25E5%25AE%25B6,</span><br><span class="line">2,1.html?lang=c&amp;stype=&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99</span><br><span class="line">&amp;jobterm=99&amp;companysize=99&amp;providesalary=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0</span><br><span class="line">&amp;confirmdate=9&amp;fromType=&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare=</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>结果不止一页，点击第二页，同样将URL复制出来：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">http://search.51job.com/list/000000,000000,0000,00,9,99,</span><br><span class="line">%25E6%2595%25B0%25E6%258D%25AE%25E7%25A7%2591%25E5%25AD%25A6%25E5%25AE%25B6,</span><br><span class="line">2,2.html?lang=c&amp;stype=1&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99</span><br><span class="line">&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0</span><br><span class="line">&amp;confirmdate=9&amp;fromType=&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare=</span><br></pre></td></tr></table></figure></p>
<p>很容易发现，这两段url唯一的不同在于”.html”前面的数字1和2，因此它代表了页码。其中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%25E6%2595%25B0%25E6%258D%25AE%25E7%25A7%2591%25E5%25AD%25A6%25E5%25AE%25B6</span><br></pre></td></tr></table></figure></p>
<p>是一种URL编码，翻译成中文就是“数据科学家”，转换方式可以使用urllib库中的quote()方法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import urllib.quote</span><br><span class="line">keyword = &apos;数据科学家&apos;</span><br><span class="line">url = quote(keyword)</span><br></pre></td></tr></table></figure></p>
<p>我们可以通过第一次的搜索结果获取页码数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def GetPages(keyword):</span><br><span class="line">    keyword = quote(keyword, safe=&apos;/:?=&apos;)</span><br><span class="line">    url = &apos;http://search.51job.com/jobsearch/search_result.php?fromJs=1&amp;jobarea=000000%2C00&amp;district=000000&amp;funtype=0000&amp;industrytype=00&amp;issuedate=9&amp;providesalary=99&amp;keyword=&apos;+keyword + \</span><br><span class="line">      &apos;&amp;keywordtype=2&amp;curr_page=1&amp;lang=c&amp;stype=1&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;list_type=0&amp;fromType=14&amp;dibiaoid=0&amp;confirmdate=9&apos;</span><br><span class="line">    html = requests.get(url)</span><br><span class="line">    soup = BeautifulSoup(html.content, &quot;html.parser&quot;)</span><br><span class="line">    span = soup.find(&apos;div&apos;, class_=&apos;p_in&apos;).find(&apos;span&apos;, class_=&apos;td&apos;)</span><br><span class="line">    page_num = span.get_text().replace(&apos;共&apos;, &apos;&apos;).replace(&apos;页，到第&apos;, &apos;&apos;)</span><br><span class="line">    return page_num</span><br></pre></td></tr></table></figure></p>
<p>由此，便可实现针对特定关键词的所有搜索结果的页面的遍历。</p>
<ul>
<li><p>URL列表构建<br>打开搜索结果页面，我们会发现，点击职位名称可以链接到每个职位的详情页面，也正是我们所需要的数据源。因此，我们只需要获取所有的搜索结果中的职位名称的超链接地址，便可以遍历所有职位的详细数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def GetUrls(keyword, page_num):</span><br><span class="line">    keyword = quote(keyword, safe=&apos;/:?=&apos;)</span><br><span class="line">    urls = []</span><br><span class="line">    p = page_num+1</span><br><span class="line">    for i in range(1, p):</span><br><span class="line">        url = &apos;http://search.51job.com/jobsearch/search_result.php?fromJs=1&amp;jobarea=000000%2C00&amp;district=000000&amp;funtype=0000&amp;industrytype=00&amp;issuedate=9&amp;providesalary=99&amp;keyword=&apos;+keyword + \</span><br><span class="line">            &apos;&amp;keywordtype=2&amp;curr_page=&apos; + \</span><br><span class="line">            str(i) + \</span><br><span class="line">            &apos;&amp;lang=c&amp;stype=1&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;list_type=0&amp;dibiaoid=0&amp;confirmdate=9&apos;</span><br><span class="line">        html = requests.get(url)</span><br><span class="line">        soup = BeautifulSoup(html.content, &quot;html.parser&quot;)</span><br><span class="line">        ps = soup.find_all(&apos;p&apos;, class_=&apos;t1&apos;)</span><br><span class="line">        for p in ps:</span><br><span class="line">            a = p.find(&apos;a&apos;)</span><br><span class="line">            urls.append(str(a[&apos;href&apos;]))</span><br><span class="line">        s = random.randint(5, 30)</span><br><span class="line">        print(str(i)+&apos;page done,&apos;+str(s)+&apos;s later&apos;)</span><br><span class="line">        time.sleep(s)</span><br><span class="line">    return urls</span><br></pre></td></tr></table></figure>
</li>
<li><p>数据请求构建<br>在获取了所有的职位数据的url之后，我们使用requests访问这些url发现，并不能顺利获取数据。因此，可以考虑在请求中加入headers数据，其中包含cookie和User_Agent：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">User_Agent = &apos;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36&apos;</span><br><span class="line">cookie = &apos;guid=14842945278988500031; slife=indexguide%3D1&apos;</span><br><span class="line">headers = &#123;&apos;User-Agent&apos;: User_Agent, &apos;cookie&apos;: cookie&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>这样，可以成功请求每个职位的详情页面数据：</p>
<ul>
<li>数据解析<br>数据解析首先是明确数据需求，这里我们将数据尽可能多的抓取下来。<br>以职位要求一栏为例，我们通过访问多个页面对比发现，这一栏可能显示的要求个数不一样：</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/5759501-c5fa43f51e6339eb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里包括了经验、学历、招聘人数和发布时间</p>
<p><img src="http://upload-images.jianshu.io/upload_images/5759501-92464313ba4cba7a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而这里则没有对于经验的要求。<br>利用浏览器开发者选项功能，查看这一栏的源码：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/5759501-00d99dc97d831f12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>这里职位的要求都放在一个class=”sp4”的span中，通过查找功能可以发现没有其他的class=”sp4”的标签，所以我们利用find_all()方法可以轻松定位到这些职位要求数据。</p>
<p>通过比较可以发现这最多的要求个数为4，所以在个数不确定的情况下，可以先新建一个包含四个空字符串元素的新数组，将所有的要求个数填入该数组，这样可以保证不同网页的数据都能获取完整。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spans = soup.find_all(&apos;span&apos;, class_=&apos;sp4&apos;)</span><br><span class="line">num = len(spans)</span><br><span class="line">nav = [&apos;&apos;, &apos;&apos;, &apos;&apos;, &apos;&apos;]</span><br><span class="line">for i in range(0, num-1):</span><br><span class="line">    nav[i] = spans[i].get_text().strip()</span><br></pre></td></tr></table></figure></p>
<p>完整代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">from urllib.parse import quote</span><br><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import time</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def GetPages(keyword):</span><br><span class="line">    keyword = quote(keyword, safe=&apos;/:?=&apos;)</span><br><span class="line">    url = &apos;http://search.51job.com/jobsearch/search_result.php?fromJs=1&amp;jobarea=000000%2C00&amp;district=000000&amp;funtype=0000&amp;industrytype=00&amp;issuedate=9&amp;providesalary=99&amp;keyword=&apos;+keyword + \</span><br><span class="line">        &apos;&amp;keywordtype=2&amp;curr_page=1&amp;lang=c&amp;stype=1&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;list_type=0&amp;fromType=14&amp;dibiaoid=0&amp;confirmdate=9&apos;</span><br><span class="line">    html = requests.get(url)</span><br><span class="line">    soup = BeautifulSoup(html.content, &quot;html.parser&quot;)</span><br><span class="line">    span = soup.find(&apos;div&apos;, class_=&apos;p_in&apos;).find(&apos;span&apos;, class_=&apos;td&apos;)</span><br><span class="line">    page_num = span.get_text().replace(&apos;共&apos;, &apos;&apos;).replace(&apos;页，到第&apos;, &apos;&apos;)</span><br><span class="line">    return page_num</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def GetUrls(keyword, page_num):</span><br><span class="line">    keyword = quote(keyword, safe=&apos;/:?=&apos;)</span><br><span class="line">    urls = []</span><br><span class="line">    p = page_num+1</span><br><span class="line">    for i in range(1, p):</span><br><span class="line">        url = &apos;http://search.51job.com/jobsearch/search_result.php?fromJs=1&amp;jobarea=000000%2C00&amp;district=000000&amp;funtype=0000&amp;industrytype=00&amp;issuedate=9&amp;providesalary=99&amp;keyword=&apos;+keyword + \</span><br><span class="line">            &apos;&amp;keywordtype=2&amp;curr_page=&apos; + \</span><br><span class="line">            str(i) + \</span><br><span class="line">            &apos;&amp;lang=c&amp;stype=1&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;list_type=0&amp;dibiaoid=0&amp;confirmdate=9&apos;</span><br><span class="line">        html = requests.get(url)</span><br><span class="line">        soup = BeautifulSoup(html.content, &quot;html.parser&quot;)</span><br><span class="line">        ps = soup.find_all(&apos;p&apos;, class_=&apos;t1&apos;)</span><br><span class="line">        for p in ps:</span><br><span class="line">            a = p.find(&apos;a&apos;)</span><br><span class="line">            urls.append(str(a[&apos;href&apos;]))</span><br><span class="line">        s = random.randint(5, 30)</span><br><span class="line">        print(str(i)+&apos;page done,&apos;+str(s)+&apos;s later&apos;)</span><br><span class="line">        time.sleep(s)</span><br><span class="line">    return urls</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def GetContent(url, headers):</span><br><span class="line">    html = requests.get(url, headers=headers)</span><br><span class="line">    soup = BeautifulSoup(html.content, &quot;html.parser&quot;)</span><br><span class="line">    PositionTitle = str(soup.find(&apos;h1&apos;)[&apos;title&apos;])</span><br><span class="line">    Location = soup.find(&apos;span&apos;, class_=&apos;lname&apos;).string</span><br><span class="line">    Salary = soup.find(&apos;strong&apos;).string</span><br><span class="line">    CompanyName = soup.find(&apos;p&apos;, class_=&apos;cname&apos;).get_text().strip()</span><br><span class="line">    CompanyType = soup.find(</span><br><span class="line">        &apos;p&apos;, class_=&apos;msg ltype&apos;).get_text().strip().replace(&apos; &apos;, &apos;&apos;).replace(&apos;  &apos;, &apos;&apos;).replace(&apos;  &apos;, &apos;&apos;).replace(&apos;  &apos;, &apos;&apos;)</span><br><span class="line">    spans = soup.find_all(&apos;span&apos;, class_=&apos;sp4&apos;)</span><br><span class="line">    num = len(spans)</span><br><span class="line">    nav = [&apos;&apos;, &apos;&apos;, &apos;&apos;, &apos;&apos;]</span><br><span class="line">    for i in range(0, num-1):</span><br><span class="line">        nav[i] = spans[i].get_text().strip()</span><br><span class="line">    Exp = nav[0]</span><br><span class="line">    Degree = nav[1]</span><br><span class="line">    RecruitNum = nav[2]</span><br><span class="line">    PostTime = nav[3]</span><br><span class="line">    Welfare = soup.find(&apos;p&apos;, class_=&apos;t2&apos;)</span><br><span class="line">    if str(type(Welfare)) == &quot;&lt;class &apos;NoneType&apos;&gt;&quot;:</span><br><span class="line">        Welfare = &apos;&apos;</span><br><span class="line">    else:</span><br><span class="line">        Welfare = Welfare.get_text().strip().replace(&apos;\n&apos;, &apos;|&apos;)</span><br><span class="line">    PositionInfo = soup.find(</span><br><span class="line">        &apos;div&apos;, class_=&apos;bmsg job_msg inbox&apos;).get_text().strip().replace(&apos;\n&apos;, &apos;&apos;).replace(&apos;分享&apos;, &apos;&apos;).replace(&apos;举报&apos;, &apos;&apos;).replace(&apos;  &apos;, &apos;&apos;).replace(&apos; &apos;, &apos;&apos;).replace(&apos;   &apos;, &apos;&apos;).replace(&apos;    &apos;, &apos;&apos;).replace(&apos;\r&apos;, &apos;&apos;)</span><br><span class="line">    PositionType = soup.find(&apos;span&apos;, class_=&apos;el&apos;)</span><br><span class="line">    if str(type(PositionType)) == &quot;&lt;class &apos;NoneType&apos;&gt;&quot;:</span><br><span class="line">        PositionType = &apos;&apos;</span><br><span class="line">    else:</span><br><span class="line">        PositionType = PositionType.get_text().strip().replace(&apos;\n&apos;, &apos;&apos;)</span><br><span class="line">    Contact = soup.find(&apos;div&apos;, class_=&apos;bmsg inbox&apos;)</span><br><span class="line">    if str(type(Contact)) == &quot;&lt;class &apos;NoneType&apos;&gt;&quot;:</span><br><span class="line">        Contact = &apos;&apos;</span><br><span class="line">    else:</span><br><span class="line">        Contact = Contact.get_text().strip().replace(</span><br><span class="line">            &apos;   &apos;, &apos;&apos;).replace(&apos;    &apos;, &apos;&apos;).replace(&apos;地图&apos;, &apos;&apos;).replace(&apos;\n&apos;, &apos;&apos;)</span><br><span class="line">    ConpanyInfo = soup.find(&apos;div&apos;, class_=&apos;tmsg inbox&apos;)</span><br><span class="line">    if str(type(ConpanyInfo)) == &quot;&lt;class &apos;NoneType&apos;&gt;&quot;:</span><br><span class="line">        ConpanyInfo = &apos;&apos;</span><br><span class="line">    else:</span><br><span class="line">        ConpanyInfo = ConpanyInfo.get_text().strip().replace(</span><br><span class="line">            &apos;\n&apos;, &apos;&apos;).replace(&apos;  &apos;, &apos;&apos;).replace(&apos; &apos;, &apos;&apos;)</span><br><span class="line">    try:</span><br><span class="line">        record = PositionTitle+&apos;\t&apos;+Location+&apos;\t&apos;+Salary+&apos;\t&apos;+CompanyName+&apos;\t&apos;+CompanyType+&apos;\t&apos;+Exp+&apos;\t&apos;+Degree+&apos;\t&apos; + \</span><br><span class="line">            RecruitNum+&apos;\t&apos;+PostTime+&apos;\t&apos;+Welfare+&apos;\t&apos;+PositionInfo + \</span><br><span class="line">            &apos;\t&apos;+str(PositionType)+&apos;\t&apos;+str(Contact)+&apos;\t&apos;+str(ConpanyInfo)</span><br><span class="line">    except Exception as e:</span><br><span class="line">        record = &apos;&apos;</span><br><span class="line">    else:</span><br><span class="line">        pass</span><br><span class="line">    finally:</span><br><span class="line">        pass</span><br><span class="line">    return record</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    with open(&apos;keywords.txt&apos;, &apos;r&apos;, encoding=&apos;utf-8&apos;) as f:</span><br><span class="line">        keywords = f.readlines()</span><br><span class="line">    for keyword in keywords[1:]:</span><br><span class="line">        keyword = keyword.strip()</span><br><span class="line">        page_num = int(GetPages(keyword))</span><br><span class="line">        urls = GetUrls(keyword, page_num)</span><br><span class="line">        with open(keyword+&apos;urls.txt&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;) as f:</span><br><span class="line">            for url in urls:</span><br><span class="line">                f.write(url+&apos;\n&apos;)</span><br><span class="line">        User_Agent = &apos;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36&apos;</span><br><span class="line">        cookie = &apos;guid=14842945278988500031; slife=indexguide%3D1&apos;</span><br><span class="line">        headers = &#123;&apos;User-Agent&apos;: User_Agent, &apos;cookie&apos;: cookie&#125;</span><br><span class="line">        with open(keyword+&apos;urls.txt&apos;, &apos;r&apos;, encoding=&apos;utf-8&apos;) as f:</span><br><span class="line">            urls = f.readlines()</span><br><span class="line">        records = []</span><br><span class="line">        i = 0</span><br><span class="line">        for url in urls:</span><br><span class="line">            url = url.strip()</span><br><span class="line">            if url != &apos;&apos;:</span><br><span class="line">                records.append(</span><br><span class="line">                    GetContent(url, headers))</span><br><span class="line">                i += 1</span><br><span class="line">                s = random.randint(5, 30)</span><br><span class="line">                print(str(i)+&apos;page done,&apos;+str(s)+&apos;s later&apos;)</span><br><span class="line">                time.sleep(s)</span><br><span class="line">        with open(keyword+&apos;.txt&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;) as f:</span><br><span class="line">            for re in records:</span><br><span class="line">                f.write(re+&apos;\n&apos;)</span><br><span class="line">        print(keyword+&apos; Done---------------------------&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/05/18/spider-recruits/" data-id="cjharcito0001xsu4dwec0m1c" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2018/05/17/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/05/18/spider-recruits/">(no title)</a>
          </li>
        
          <li>
            <a href="/2018/05/17/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Liu Huan<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>